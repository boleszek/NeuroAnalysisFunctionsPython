"""
Created on Sat Jun 24, 2017
@author: Boleszek Osinski, Kay Lab, University of Chicago

These functions were originaly designed to analyze spikes and LFPs
- detected with 32-channel 2-shank Si probes (Cambridge Neurotech)
- digitaly recorded using OpenEphys software

Data formats:
- Continuous data must be converted to flat binary .dat format to be analyzed
- Spike timing data are stroded in hdf5 format kwik files generated by klustakwik2

"""

import numpy as np
from scipy.signal import butter, hilbert, filtfilt, freqz, coherence
from scipy import stats
import random



def load_spikes_from_kwik(dfile, clust_choice, use_clust_choice):
    '''
    Inputs : dfile        - file directory (including filename)
             clust_choice - list of cluster labels you want to extract
             use_clust_choice - if 1 t
        
    Outputs: SPK          - list of spike times (in samples) for each cluster
    '''
    import h5py
    with h5py.File(dfile,'r') as D:
        # for viewing contents of folders
        # dv=D['/channel_groups/0/spikes/time_samples']
        # for i in iter(dv):
            # print(i)
        time_samples = D.get('/channel_groups/0/spikes/time_samples')
        np_time_samples = np.array(time_samples)
        cluster_labels = D.get('/channel_groups/0/spikes/clusters/main')
        np_cluster_labels = np.array(cluster_labels) # label of each individual spike
        cluster_names = D.get('/channel_groups/0/clusters/main')
        np_cluster_names = np.array(cluster_names) # name of each cluster category
        # must additionally convert from <U2 to int32
        np_cluster_names = np_cluster_names.astype('int32')
        
    SPK = []
    if use_clust_choice == 1:
        for i in clust_choice:
            temp_ind = np.squeeze(np.array(np.where(np_cluster_labels == i)))
            SPK.append(np_time_samples[temp_ind])
        # redefine cluster names as the clust_choice, since those are the clusters we chose, duh
        np_cluster_names = clust_choice.copy()
    else:
        for i in range(len(np_cluster_names)):
            temp_ind = np.squeeze(np.array(np.where(np_cluster_labels == np_cluster_names[i])))
            SPK.append(np_time_samples[temp_ind])
    return SPK, np_cluster_names

def find_matching_files(ddir,match_str_list):
    # This function finds all the folders in data directory ddir with names that includes the entries in
    # match_str_list
    import os
    ddir_list = os.listdir(ddir)
    dfold_list = []
    for file in ddir_list:
        for i in range(len(match_str_list)):
            if match_str_list[i] in file:
                dfold_list.append(file)
    return dfold_list

def merge_clust(SPK,clust_names,merge,cchan=[]):
    # This function merges the clustets listed in merge.
    # It deletes the individual clusters and appends the new one
    # INPUTS:
    # SPK         -  list of spike times
    # clust_names -  list of cluster names (ordered in same way as SPK)
    # merge       -  list of clusters to be merged
    # cchan       -  cluster channel. This is optional since we may not need it in some cases
    ####################################
    
    # merge to the first cluster in group
    ord_inds = []
    for i in range(len(merge)):
        temp_inds = []
        for j in merge[i]:
            temp_inds.append(clust_names.index(j))
            ord_inds.append(clust_names.index(j)) # also add to ord_inds for later deletion
        SPK_merged = []
        for d in temp_inds:
            SPK_merged = np.sort(np.hstack((SPK_merged,SPK[d])))
        # append merged cluster to SPK list and new clust name
        SPK.append(SPK_merged)
        clust_names.append(merge[i])

    # only edit cchan if it is supplied as an ndarray
    if isinstance(cchan,np.ndarray):
        for i in range(len(merge)):
            # the chan of first cluster should be same as all others in a merge unit. Just use first!
            cchan = np.append(cchan,cchan[clust_names.index(merge[i][0])]) # must use np.append for arrays
        ord_inds = sorted(ord_inds,reverse=True) # sort from largest to smallest to facilitate deletion
        for d in ord_inds:
            cchan = np.delete(cchan,d)
    
    # delete the unmereged clusters
    ord_inds = sorted(ord_inds,reverse=True) # sort from largest to smallest to facilitate deletion
    for d in ord_inds:
        del SPK[d]
        del clust_names[d]
    
    return SPK, clust_names, cchan

def load_waveforms_with_merge_and_down(foldername,cl_chan,cl_names,dlim):
    # - This function loads spike waveforms and masks that were exported by get_spike_waveforms_and_masks.py
    # - It assumes .npy files with 'wv_clust' in the waveform filename and 'msk_clust' in the mask filename.
    # - If multiple cl_names exist it will merge their waveforms
    #
    # Dependencies:
    # find_matching_files
    #
    # INPUTS:
    # foldername    -  folder where files are stored
    # cl_chan       -  the channel that the clusters refer to (a single number)
    # cl_names      -  the names of the clusters (can be > 1)
    # dlim          -  downsampling limit (if nspikes > dlim), then downsample to dlim spikes
    ############################################################################
    
    if isinstance(cl_names,list):
        # need to merge waveforms
        WFfile_list = find_matching_files(foldername,['wv_chan'+str(cl_chan)])
        MSKfile_list = find_matching_files(foldername,['msk_chan'+str(cl_chan)])
        WV_list = []
        MSK_list = []
        for c in range(len(cl_names)):
            wv_name = WFfile_list[c]
            msk_name = MSKfile_list[c]
            WV_list.append(np.load(foldername+'/'+wv_name))
            MSK_list.append(np.load(foldername+'/'+msk_name))
        # concatenate waveforms and masks (this effectively merges them, although doesn't preserve temporal order)
        WV = WV_list[0] #initialize
        MSK = MSK_list[0] #initialize
        for i in np.arange(1,len(cl_names),1):
            WV = np.vstack((WV,WV_list[i]))
            MSK = np.vstack((MSK,MSK_list[i]))
    else:
        # No need to merge waveforms
        # The output of find_matching_files is inside of a list, so I have to index by [0]
        wv_name = find_matching_files(foldername,['wv_chan'+str(cl_chan)+'_clust'+str(cl_names)])[0]
        msk_name = find_matching_files(foldername,['msk_chan'+str(cl_chan)+'_clust'+str(cl_names)])[0]
        WV = np.load(foldername+'/'+wv_name)
        MSK = np.load(foldername+'/'+msk_name)

    ns = len(MSK)
    if ns > dlim:
        # take a random, NON REPEATING, sample of the data of size dlim
        rand_inds = random.sample(range(ns), dlim) # this function is awesome! Non-repeating :)
        WV = WV[rand_inds,:,:]
        MSK = MSK[rand_inds,:]

    return WV, MSK


def trim_events(odor_periods,trim):
    # odor periods    -    list of all odor periods in samples (start and stop are consecutive)
    # trim            -    list of start and stop samples of noisy periods, ordered from lateset to earliest
    
    from itertools import compress # used for indexing with booleans
    
    ALLstart = odor_periods[0::2]
    ALLstop = odor_periods[1::2]
    for i in range(len(trim)):
        # detect overlap of odor periods with trim periods (hopefully there aren't any!)
        ovlap = list(compress(ALLstart, (np.array(ALLstart) > trim[i][0]) & (np.array(ALLstart) < trim[i][1])))
        if ovlap != []:
            print('Overlap detected! Setting odor period start '+str(ovlap)+' to trim start '+str(tr[i][0])+' -1')
            yo=np.where(np.array(ALLstart) == ovlap)
            ALLstart[int(yo[0])] = trim[i][0]-1 # add -1 so that there is no overlap
        # subtract away the trimmed period
        ALLstart_shift = [a - (trim[i][1]-trim[i][0]) for a in ALLstart if a > trim[i][1]] # downshift ALLstart > trim
        ALLstart = list(compress(ALLstart,np.array(ALLstart) < trim[i][1])) # redifeine ALLstart as values < trim
        ALLstart = ALLstart + ALLstart_shift # add the 1st and 2nd parts of ALLstart
        
        ovlap = list(compress(ALLstop, (np.array(ALLstop) > trim[i][0]) & (np.array(ALLstop) < trim[i][1])))
        if ovlap != []:
            print('Overlap detected! Setting odor period stop '+str(ovlap)+' to trim start '+str(tr[i][0])+' -1')
            yo=np.where(np.array(ALLstop) == ovlap)
            ALLstop[int(yo[0])] = trim[i][0]-1 # add -1 so that there is no overlap
        # subtract away the trimmed period
        ALLstop_shift = [a - (trim[i][1]-trim[i][0]) for a in ALLstop if a > trim[i][1]] # downshift ALLstop > trim
        ALLstop = list(compress(ALLstop,np.array(ALLstop) < trim[i][1])) # redifeine ALLstop as values < trim
        ALLstop = ALLstop + ALLstop_shift # add the 1st and 2nd parts of ALLstop
            
    # redefine odor periods
    odor_periods[0::2] = np.array(ALLstart)
    odor_periods[1::2] = np.array(ALLstop)
    return odor_periods

def get_FR(spikes,bw,sf,maxt):
    # spikes     -     vector of spike times
    # bw         -     bin window (s)
    # sf         -     sampling fq
    # maxt       -     max time bin
    bins = np.arange(0,maxt,bw)
    [fr,bleh] = np.histogram(spikes/sf,bins)
    fr = fr/bw # convert counts to rate
    return fr

def fft_peak_power_in_band(signal,Fs,fstart,fstop):
    # This function calculates FFT power spectrum of signal and calculates peak power in chosen fq band
    # INPUTS:
    # signal   - continuous signal
    # Fs       - sampling frequency (Hz)
    # fstart       - start of fq band
    # fstop       - stop of fq band
    ######################################
    N = len(signal)
    T = 1.0 / Fs #(s)
    signal_FFT = np.fft.fft(signal)
    signal_PWR = 2.0/N * np.abs(signal_FFT[0:int(N/2)])
    
    f = np.linspace(0.0, 1.0/(2.0*T), N/2) # define freuqency vector
    fstart_ind = np.where(f>fstart)[0][0]
    fstop_ind = np.where(f>fstop)[0][0]
    
    maxpwr = np.max(signal_PWR[fstart_ind:fstop_ind]) # max power
    
    return signal_PWR, maxpwr, f, fstart_ind, fstop_ind

def fft_integrate_power_in_band(signal,Fs,fstart,fstop):
    # This function calculates FFT power spectrum of signal and integrates the power in chosen fq band
    # INPUTS:
    # signal   - continuous signal
    # Fs       - sampling frequency (Hz)
    # fstart       - start of fq band
    # fstop       - stop of fq band
    ######################################
    N = len(signal)
    T = 1.0 / Fs #(s)
    signal_FFT = np.fft.fft(signal)
    signal_PWR = 2.0/N * np.abs(signal_FFT[0:int(N/2)])
    
    f = np.linspace(0.0, 1.0/(2.0*T), N/2) # define freuqency vector
    fstart_ind = np.where(f>fstart)[0][0]
    fstop_ind = np.where(f>fstop)[0][0]
    
    sumpwr = np.sum(signal_PWR[fstart_ind:fstop_ind]) # sum power
    
    return signal_PWR, sumpwr, f, fstart_ind, fstop_ind

def butter_env(order, low, high, signal):
    # This function bandpass filters the signal between low and high frequencies using butterworth filter of chosen order.
    # Returns filtered signal, envelope, and phase determined by hilbert transform
    # INPUTS:
    # order    - order of butterworth filter
    # low      - low lim of fq band (Hz/nyq)
    # high     - high lim of fq band (Hz/nyq)
    # signal   - signal to be filtered
    # NOTE: low and high fq band cutoffs must be divided by nyquist frequncy!
    ######################################
    b, a = butter(order, [low, high], btype='bandpass')
    filt_signal = filtfilt(b, a, signal)
    # Amplitude of analytic signal is the envelope
    hilb_filt_signal = hilbert(filt_signal)
    filt_signal_env = np.abs(hilb_filt_signal)
    # The instantaneous phase corresponds to the phase angle of the analytic signal
    filt_signal_ph = np.angle(hilb_filt_signal)
    return filt_signal, filt_signal_env, filt_signal_ph

def spike_field_prod_with_rand(spikes,field,nrand):
    # This function indexes field by spikes AND by nrand random arrays the same length as spikes
    # NOTE: this function assumes that spikes are sampled 10x higher than field
    # spike sample times must be subsampled and rounded to index ssLFP envelope
    ssinds = np.rint(0.1*np.array(spikes)) # round to nearest integer
    ssinds = ssinds.astype(int) # convert type to int
    SFprod = field[ssinds]
    # Do the same for nrand randomly generated spike trains as a control
    RFprod = [] # initialize list that will store nrand random spike-field products
    for n in range(nrand):
        randinds = field.size*np.random.sample(spikes.size)
        randinds = randinds.astype(int) # convert type to int
        RFprod.append(field[randinds])
    return SFprod, RFprod

def spike_field_coh(spike_times,field,sssf):
    # This function uses the scipy.signal.coherence function. Must be imported
    # Assuming spike times are given in units of samples.
    # Also assuming LFP is subsampled by 10
    # This function bins spike times in bins of LFP sampling time, then computes coherence
    bw = 10
    bins = np.arange(0,10*len(field),bw)
    [binned_spikes,bleh] = np.histogram(spike_times,bins)
    if len(binned_spikes) < len(field): # ensure binned spikes and LFP are same len
        binned_spikes=np.append(binned_spikes,0) # add zero to end
    elif len(binned_spikes) > len(field):
        binned_spikes=np.delete(binned_spikes,0) # delete 1st element
    f, Cxy = coherence(binned_spikes, field, sssf, nperseg=2000)
    # higher nperseg --> finer fq resolution
    return f, Cxy

def SPH_wrapper(SPK_op,LFPb_ph_op,sssf,no,nop,startb_op,stopb_op,nrand):
    # This function calculates spike phase product between spike times and LFP phase
    # The function is adapted to deal with single odorant and multiple interleaved odorant data
    #########################################
    # INPUTS:
    # SPK_op              -  embeded list of spike times during odor pres (ordered by 1st index pres, 2nd index cell)
    # LFPb_ph_op          -  embeded list of LFP phase (ordered by 1st index pres, 2nd index cell)
    # sssf                -  sub sampled sampling frequency
    # no                  -  number of odors
    # nop                 -  number of presentations
    # startb_op           -  start of thresholded periods
    # stop_op             -  stop of thresholded periods
    # nrand               -  number of spike shuffles for random phase noise floor
    #########################################

    ncell = len(SPK_op[0])# number of cells
    
    if no == 1:
        SPHprod = list() # SPHprod will have length = len(SPK)
        RPHprod = list() # same as SPHprod but for random spikes
        for s in range(ncell):
            SPHprod_temp = list()
            RPHprod_temp = list()
            for i in range(nop):
                # extract spikes for each start/stop pair of the thresholded region
                for b in range(startb_op[i].size):
                    if np.isscalar(startb_op[i]): # if scalar don't index by b (only one beta in this op)
                        current_start = startb_op[i] # only saving this to realign spikes for 1D cat
                        bool_array =(SPK_op[i][s] > startb_op[i]) & (SPK_op[i][s] < stopb_op[i])
                    else:
                        current_start = startb_op[i][b] # only saving this to realign spikes for 1D cat
                        bool_array =(SPK_op[i][s] > startb_op[i][b]) & (SPK_op[i][s] < stopb_op[i][b])
                    sind = np.array(np.where(np.squeeze(bool_array)))
                    if sind.size > 0: # trouble dealing with empty arrays
                        # concatenate spike-phase products
                        [SFprod,RFprod] = spike_field_prod_with_rand(SPK_op[i][s][0][sind],LFPb_ph_op[i],nrand)
                        SPHprod_temp.append(SFprod)
                        RPHprod_temp.append(RFprod)
            # save spike-phase products for each cluster
            if len(SPHprod_temp) == 0:
                # np.hstack is unable to deal with empty entries (stupid)
                SPHprod.append(np.empty( shape=(0, 0) )) # must add empty ARRAY (not LIST) for later processing
                RPHprod.append(np.empty( shape=(0, 0) )) # must add empty ARRAY (not LIST) for later processing
            else:
                SPHprod.append(np.hstack(SPHprod_temp))
                RPHprod.append(np.hstack(RPHprod_temp))
    else:
    
        SPHprod = []
        RPHprod = []
        for o in range(no):
            SPHprod_o_temp = list() # SPHprod_o_temp will have length = len(SPK)
            RPHprod_o_temp = list() # same as SPHprod_o_temp but for random spikes
            inds = np.arange(o,nop*no,no)
            for s in range(ncell):
                SPHprod_temp = list()
                RPHprod_temp = list()
                for i in range(nop):
                    # extract spikes for each start/stop pair of the thresholded region
                    for b in range(startb_op[o][i].size):
                        if np.isscalar(startb_op[o][i]): # if scalar don't index by b (only one beta in this op)
                            current_start = startb_op[o][i] # only saving this to realign spikes for 1D cat
                            bool_array =(SPK_op[inds[i]][s] > startb_op[o][i]) & (SPK_op[inds[i]][s] < stopb_op[o][i])
                        else:
                            current_start = startb_op[o][i][b] # only saving this to realign spikes for 1D cat
                            bool_array =(SPK_op[inds[i]][s] > startb_op[o][i][b]) & (SPK_op[inds[i]][s] < stopb_op[o][i][b])
                        sind = np.array(np.where(np.squeeze(bool_array)))
                        if sind.size > 0: # trouble dealing with empty arrays
                            # concatenate spike-phase products
                            [SFprod,RFprod] = spike_field_prod_with_rand(SPK_op[inds[i]][s][0][sind],LFPb_ph_op[inds[i]],nrand)
                            SPHprod_temp.append(SFprod)
                            RPHprod_temp.append(RFprod)
                # save spike-phase products for each cluster
                if len(SPHprod_temp) == 0:
                    # np.hstack is unable to deal with empty entries (stupid)
                    SPHprod_o_temp.append(np.empty( shape=(0, 0) )) # must add empty ARRAY (not LIST) for later processing
                    RPHprod_o_temp.append(np.empty( shape=(0, 0) )) # must add empty ARRAY (not LIST) for later processing
                else:
                    SPHprod_o_temp.append(np.hstack(SPHprod_temp))
                    RPHprod_o_temp.append(np.hstack(RPHprod_temp))
            SPHprod.append(SPHprod_o_temp)
            RPHprod.append(RPHprod_o_temp)

    return SPHprod, RPHprod


def SFC_wrapper(SPK_op,LFPb_op,sssf,no,nop,use_full_op_for_sfc,startb_op,stopb_op,nrand):
    # This is a wrapper for spike_field_coh. It accepts single odorant and multiple odorant data,
    # where no is number of odorants. This wrapper also alows you to choose to use the full odor
    # presentation period or a specified list of periods (usually where beta power is a bove a threshold)
    # for calculating SFC, by seting use_full_op_for_sfc to 1 or 0
    #########################################
    # INPUTS:
    # SPK_op              -  embeded list of spike times during odor pres (ordered by 1st index pres, 2nd index cell)
    # LFPb_op             -  embeded list of filtered LFP (ordered by 1st index pres, 2nd index cell)
    # sssf                -  sub sampled sampling frequency
    # no                  -  number of odors
    # nop                 -  number of presentations
    # use_full_op_for_sfc -  if=1 use spikes from entire presentation period, if 0 use spikes from thresholded periods
    # startb_op           -  start of thresholded periods (only used if use_full_op_for_sfc = 1)
    # stop_op             -  stop of thresholded periods (only used if use_full_op_for_sfc = 1)
    # nrand               -  number of spike shuffles for SFC noise floor
    #########################################
    
    nspk = len(SPK_op[0]) # number of cells
    
    # if using a single odorant we don't need extra for loop (which may generate empty dimensions in arrays)
    if no == 1:
        
        # first we must concatenate all beta or odor periods and spike times into long vectors
        
        if use_full_op_for_sfc == 0:
            # use beta thresholded regions only!
            LFPb_cat = np.empty(0) # initialize array which will concatenate all beta periods
            SPKb_cat = [None]*nspk
            for s in range(nspk):
                cumulative_time_shift = 0 # cumulative shift in time to concatenate spike times because each op starts from 0
                SPKb_cat[s] = np.empty(0)
                for i in range(nop):
                    # extract spikes for each start/stop pair of the thresholded region
                    for b in range(startb_op[i].size):
                        if np.isscalar(startb_op[i]): # if scalar don't index by b (only one beta in this op)
                            current_start = startb_op[i] # only saving this to realign spikes for 1D cat
                            bool_array =(SPK_op[i][s] > startb_op[i]) & (SPK_op[i][s] < stopb_op[i])
                            if s == 0: # concatenate LFPb only for first s (since s is over spikes)
                                LFPb_cat = np.append(LFPb_cat,LFPb_op[i][int(startb_op[i]/10):int(stopb_op[i]/10)])
                        else:
                            current_start = startb_op[i][b] # only saving this to realign spikes for 1D cat
                            bool_array =(SPK_op[i][s] > startb_op[i][b]) & (SPK_op[i][s] < stopb_op[i][b])
                            if s == 0: # concatenate LFPb only for first s (since s is over spikes)
                                LFPb_cat = np.append(LFPb_cat,LFPb_op[i][int(startb_op[i][b]/10):int(stopb_op[i][b]/10)])
                        sind = np.array(np.where(np.squeeze(bool_array)))
                        if sind.size > 0: # trouble dealing with empty arrays
                            # concatenate spike times into single vector (used for SFC)
                            SPKb_cat[s] = np.append(SPKb_cat[s],SPK_op[i][s][0][sind]+cumulative_time_shift-current_start)
                        if np.isscalar(startb_op[i]): # update time_shift
                            cumulative_time_shift = cumulative_time_shift+stopb_op[i]-startb_op[i]
                        else:
                            cumulative_time_shift = cumulative_time_shift+stopb_op[i][b]-startb_op[i][b]
        # NOTE: must subtract 2 different start times! Current start, and past start!
        else:
            # use full op!
            
            # flatten LFP_op into single array
            LFPb_cat = np.empty(0) # initialize array which will concatenate all odor periods
            for i in range(nop):
                LFPb_cat = np.append(LFPb_cat,LFPb_op[i])
        
            SPKb_cat = [None]*nspk
            for s in range(nspk):
                cumulative_time_shift = 0 # cumulative shift in time to concatenate spike times because each op starts from 0
                SPKb_cat[s] = np.empty(0)
                for i in range(nop):
                    if SPK_op[i][s].size > 0: # trouble dealing with empty arrays
                        # concatenate spike times into single vector (used for SFC)
                        SPKb_cat[s] = np.append(SPKb_cat[s],SPK_op[i][s][0]+cumulative_time_shift)
                    # update time_shift
                    cumulative_time_shift = cumulative_time_shift+stop[i]-start[i]
                    # NOTE: must subtract 2 different start times! Current start, and past start!

        # Compute SFC (with rand)
        SFCb = []
        SFCb_max = []
        SFCb_rand = []
        SFCb_rand_max = []
        for i in range(nspk):
            f, Cxy = spike_field_coh(SPKb_cat[i],LFPb_cat,sssf)
            SFCb.append(Cxy)
            Cxyrand = [];
            for r in range(nrand):
                randSPK = 10*len(LFPb_cat)*np.random.random(SPKb_cat[i].size)
                f, Cxy = spike_field_coh(randSPK,LFPb_cat,sssf)
                Cxyrand.append(Cxy)
            SFCb_rand.append(Cxyrand)
            SFCb_max.append(np.max(SFCb[i][1:30]))
            rand_max_tmp = []
            for r in range(nrand):
                rand_max_tmp.append(np.max(SFCb_rand[i][r][1:30]))
            SFCb_rand_max.append(rand_max_tmp)

    else:
    
    # first we must concatenate all beta or odor periods and spike times into long vectors
    
        if use_full_op_for_sfc == 0:
            LFPb_cat = []
            SPKb_cat = []
            for o in range(no):
                LFPb_cat_o_temp = np.empty(0) # initialize array which will concatenate all beta periods
                SPKb_cat_o_temp = [None]*nspk
                inds = np.arange(o,nop*no,no)
                for s in range(nspk):
                    cumulative_time_shift = 0 # cumulative shift in time to concatenate spike times because each op starts from 0
                    SPKb_cat_o_temp[s] = np.empty(0)
                    for i in range(nop):
                        # extract spikes for each start/stop pair of the thresholded region
                        for b in range(startb_op[o][i].size):
                            if np.isscalar(startb_op[o][i]): # if scalar don't index by b (only one beta in this op)
                                current_start = startb_op[o][i] # only saving this to realign spikes for 1D cat
                                bool_array =(SPK_op[inds[i]][s] > startb_op[o][i]) & (SPK_op[inds[i]][s] < stopb_op[o][i])
                                if s == 0: # concatenate LFPb only for first s (since s is over spikes)
                                    LFPb_cat_o_temp = np.append(LFPb_cat_o_temp,LFPb_op[inds[i]][int(startb_op[o][i]/10):int(stopb_op[o][i]/10)])
                            else:
                                current_start = startb_op[o][i][b] # only saving this to realign spikes for 1D cat
                                bool_array =(SPK_op[inds[i]][s] > startb_op[o][i][b]) & (SPK_op[inds[i]][s] < stopb_op[o][i][b])
                                if s == 0: # concatenate LFPb only for first s (since s is over spikes)
                                    LFPb_cat_o_temp = np.append(LFPb_cat_o_temp,LFPb_op[inds[i]][int(startb_op[o][i][b]/10):int(stopb_op[o][i][b]/10)])
                            sind = np.array(np.where(np.squeeze(bool_array)))
                            if sind.size > 0: # trouble dealing with empty arrays
                                # concatenate spike times into single vector (used for SFC)
                                SPKb_cat_o_temp[s] = np.append(SPKb_cat_o_temp[s],SPK_op[inds[i]][s][0][sind]+cumulative_time_shift-current_start)
                            if np.isscalar(startb_op[o][i]): # update time_shift
                                cumulative_time_shift = cumulative_time_shift+stopb_op[o][i]-startb_op[o][i]
                            else:
                                cumulative_time_shift = cumulative_time_shift+stopb_op[o][i][b]-startb_op[o][i][b]
                                # NOTE: must subtract 2 different start times! Current start, and past start!
                # save concatenated LFBb and SPKb
                LFPb_cat.append(LFPb_cat_o_temp)
                SPKb_cat.append(SPKb_cat_o_temp)

        else:
            # use full op!
        
            # flatten LFPb_op into single array
            LFPb_cat = []
            for o in range(no):
                LFPb_cat_o_temp = np.empty(0) # initialize array which will concatenate all odor periods
                inds = np.arange(o,nop*no,no)
                for i in range(nop):
                    LFPb_cat_o_temp = np.append(LFPb_cat_o_temp,LFPb_op[inds[i]])
                LFPb_cat.append(LFPb_cat_o_temp)
        
            SPKb_cat = []
            for o in range(no):
                SPKb_cat_o_temp = [None]*nspk
                inds = np.arange(o,nop*no,no)
                for s in range(nspk):
                    cumulative_time_shift = 0 # cumulative shift in time to concatenate spike times because each op starts from 0
                    SPKb_cat_o_temp[s] = np.empty(0)
                    for i in range(nop):
                        if SPK_op[inds[i]][s].size > 0: # trouble dealing with empty arrays
                            # concatenate spike times into single vector (used for SFC)
                            SPKb_cat_o_temp[s] = np.append(SPKb_cat_o_temp[s],SPK_op[inds[i]][s][0]+cumulative_time_shift)
                        # update time_shift
                        cumulative_time_shift = cumulative_time_shift+stop[inds[i]]-start[inds[i]]
                # NOTE: must subtract duration of op. No need for current_start
            SPKb_cat.append(SPKb_cat_o_temp)

        # Compute SFC (with rand)
        SFCb = []
        SFCb_max = []
        SFCb_rand = []
        SFCb_rand_max = []
        for o in range(no):
            SFCb_o_temp = []
            SFCb_max_o_temp = []
            SFCb_rand_o_temp = []
            SFCb_rand_max_o_temp = []
            for i in range(nspk):
                f, Cxy = spike_field_coh(SPKb_cat[o][i],LFPb_cat[o],sssf)
                SFCb_o_temp.append(Cxy)
                Cxyrand = [];
                for r in range(nrand):
                    randSPK = 10*len(LFPb_cat)*np.random.random(SPKb_cat[o][i].size)
                    f, Cxy = spike_field_coh(randSPK,LFPb_cat[o],sssf)
                    Cxyrand.append(Cxy)
                SFCb_rand_o_temp.append(Cxyrand)
                SFCb_max_o_temp.append(np.max(SFCb_o_temp[i][1:30]))
                rand_max_tmp = []
                for r in range(nrand):
                    rand_max_tmp.append(np.max(SFCb_rand_o_temp[i][r][1:30]))
                SFCb_rand_max_o_temp.append(rand_max_tmp)
            SFCb.append(SFCb_o_temp)
            SFCb_max.append(SFCb_max_o_temp)
            SFCb_rand.append(SFCb_rand_o_temp)
            SFCb_rand_max.append(SFCb_rand_max_o_temp)

    return SFCb, SFCb_max, SFCb_rand, SFCb_rand_max

def exctract_LFP_trials(start,stop,nop,ssLFP):
    # INPUTS:
    # start      -       start time of extraction window (samples)
    # stop       -       stop time of extraction window (samples)
    # nop        -       number of odor presentations (usually 30)
    # ssLFP      -       sub sampled LFP

    ssLFP_op = list()
    for i in range(nop):
        # remember, LFP is subsampled by 10
        ssLFP_op.append(ssLFP[int(start[i]/10):int(stop[i]/10)])

    return ssLFP_op

def exctract_SPK_trials(start,stop,nop,SPK):
    # INPUTS:
    # start      -       start time of extraction window (samples)
    # stop       -       stop time of extraction window (samples)
    # nop        -       number of odor presentations (usually 30)
    # SPK        -       1 x nclusters list of spike times (samples)

    SPK_op = list()
    for i in range(nop):
        SPKtmp = list()
        for s in range(len(SPK)):
            # find spk times in odor periods. Don't save spikes within 10 smaples of the stop
            # to avoid later indexing issues when indexing last element of subsampled array
            sind = np.array(np.where((SPK[s] > start[i]) & (SPK[s] < (stop[i]-10))))
            sind = sind.astype(int) # convert type to int
            SPKtmp.append(SPK[s][sind]-start[i]) # subtract away start time of odor period
        SPK_op.append(SPKtmp)

    return SPK_op

def align_spikes_to_beta(no,nop,thb_sc,LFPb_env,SPK,odor_periods,sf):
    # This function aligns spike times to onset of beta oscillation determined by threshold
    # If no > 1 then the function will divide trials up into each odor
    # NOTE: This function assumes the odor presentations were interleaved if no>1!
    ######################################################
    # INPUTS:
    # no             -  number of odors
    # nop            -  number of presentations per odor
    # thb_sc         -  scaling of max beta envelope used to define threshold for beta onset
    # LFPb_env       -  List of beta envelope (per odor if no>1)
    # SPK            -  List of spike times for each cluster
    # odor_periods   -  List of start/stop times of each odor presentation
    # sf             -  Sampling freuqency
    ######################################################
    
    sssf = sf/10
    
    # Extract LFPb_env, in restricted odor periods
    # for determining acurate odor-evoked beta env integral and finding first real beta
    # trying to exclude noise-like deflections occurring at beggining of some trials
    sc1 = 0.8
    sc2 = 2
    start = odor_periods[0::2] - sc1*sf
    stop = odor_periods[1::2] + sc2*sf
    LFPb_env_op = exctract_LFP_trials(start,stop,nop*no,LFPb_env)
    #benv_integral = [np.sum(LFPb_env_op[i]) for i in range(nop*no)]
    #sindbei=np.argsort(benv_integral) # sorted inds of beta envelope integral
    # Get rid of edge effects by setting envelope on either edge = 0!!!
    # This will also allow detection of suprathresh at start of window!
    for i in range(nop*no):
        LFPb_env_op[i][0:50] = 0
        LFPb_env_op[i][-50:] = 0
    
    if no == 1:
        # Get index of threshold on beta power
        #thb = thb_sc*np.max(LFPb_env) # threshold on entire data set (good for EMB data)
        thb = thb_sc*np.median([np.max(LFPb_env_op[i]) for i in range(nop)])
        startb_op = list()
        for i in range(nop):
            yo = LFPb_env_op[i] >= thb
            # find first nonzero element (should correspond to beta onset)
            # If there is no detectable beta, then use lower thresh
            if sum(yo.nonzero()[0]) == 0:
                yo = LFPb_env_op[i] >= thb/2
            if sum(yo.nonzero()[0]) == 0:
                yo = LFPb_env_op[i] >= thb/2.5
            if sum(yo.nonzero()[0]) == 0:
                yo = LFPb_env_op[i] >= thb/3
            startb_op.append(yo.nonzero()[0][0]) # this is in ssLFP time!
        
        
        # Extract LFP, LFPb_env, and spikes in extended odor periods (to include baseline)
        scext1 = 6 #s
        scext2 = 6
        start = odor_periods[0::2] - scext1*sf
        stop = odor_periods[1::2] + scext2*sf
        LFPb_env_op_ext = exctract_LFP_trials(start,stop,nop,LFPb_env)
        #ssLFP_op_ext = exctract_LFP_trials(start,stop,nop,ssLFP)
        SPK_op_ext = exctract_SPK_trials(start,stop,nop,SPK)
        # adjust startb_ob indices for extended windows
        startb_op_ext = [startb_op[i]+sssf*(scext1-sc1) for i in range(nop)]
        
        # Align spikes to beta env thresh crossing for extended windows
        SPK_op_ext_aligned = []
        for i in range(nop):
            SPKtmp = list()
            for s in range(len(SPK)):
                # align spikes to startb_op_ext
                SPKtmp.append(SPK_op_ext[i][s] - startb_op_ext[i]*10)
            SPK_op_ext_aligned.append(SPKtmp)
    else:
        
        # Get index of threshold on beta power for each odor
        #thb = []
        #for o in range(no):
        #    thb.append(thb_sc[o]*np.max(np.hstack(LFPb_env_op[o::no])))
        #print('using global max per odor')
        thb = []
        for o in range(no):
            # use the median of max values to minimize scaling by outliers
            thb.append(thb_sc[o]*np.median([np.max(LFPb_env_op[o::no][i]) for i in range(nop)]))
        
        startb_op = []
        for o in range(no):
            inds = np.arange(o,nop*no,no)
            startb_op_temp = []
            for i in range(nop):
                yo = LFPb_env_op[inds[i]] >= thb[o]
                # find first nonzero element (should correspond to beta onset)
                # If there is no detectable beta, then use lower thresh
                if sum(yo.nonzero()[0]) == 0:
                    yo = LFPb_env_op[inds[i]] >= thb[o]/2
                if sum(yo.nonzero()[0]) == 0:
                    yo = LFPb_env_op[inds[i]] >= thb[o]/2.5
                if sum(yo.nonzero()[0]) == 0:
                    yo = LFPb_env_op[inds[i]] >= thb[o]/3
                startb_op_temp.append(yo.nonzero()[0][0]) # this is in ssLFP time!
            startb_op.append(startb_op_temp)
        
        # Extract LFP, LFPb_env, and spikes in extended odor periods (to include baseline)
        scext1 = 6
        scext2 = 6
        start = odor_periods[0::2] - scext1*sf
        stop = odor_periods[1::2] + scext2*sf
        LFPb_env_op_ext = exctract_LFP_trials(start,stop,nop*no,LFPb_env)
        #ssLFP_op_ext = exctract_LFP_trials(start,stop,nop*no,ssLFP)
        SPK_op_ext = exctract_SPK_trials(start,stop,nop*no,SPK)
        # adjust startb_ob indices for extended windows
        startb_op_ext = []
        for o in range(no):
            startb_op_ext.append([startb_op[o][i]+sssf*(scext1-sc1) for i in range(nop)])
        
        # Align spikes to beta env thresh crossing for extended windows
        SPK_op_ext_aligned = []
        for o in range(no):
            SPK_op_ext_aligned_temp = []
            inds = np.arange(o,nop*no,no)
            for i in range(nop):
                SPKtmp = list()
                for s in range(len(SPK)):
                    # align spikes to startb_op_ext
                    # startb_op_ext is in ssLFP time!
                    SPKtmp.append(SPK_op_ext[inds[i]][s] - startb_op_ext[o][i]*10)
                SPK_op_ext_aligned_temp.append(SPKtmp)
            SPK_op_ext_aligned.append(SPK_op_ext_aligned_temp)

    return SPK_op_ext_aligned, LFPb_env_op, LFPb_env_op_ext

def calc_PSTH(no,nop,bw,edges,SPK_op_ext_aligned):
    # This function calculates PSTH for aligned spikes
    # If no > 1 then the function will divide trials up into each odor
    # NOTE: This function assumes the odor presentations were interleaved if no>1!
    ######################################################
    # INPUTS:
    # bw                 -  bin width (s)
    # edges              -  edges in time used to calculate histogram for PSTH
    # SPK_op_ext_aligned -  embedded lists of beta-aligned spikes. This is output from align_spikes_to_beta
    ######################################################
    
    if no == 1:
        PSTH = []
        PSTH_trim = [] # trim 2s from start and end of PSTH (where there are missing spikes due to shifting for alignment)
        PSTH_blmn = []
        PSTH_blstd = []
        trim = int(2/bw - 1) # number of indices of PSTH corresponding to 2s
        for s in range(len(SPK_op_ext_aligned[0])):
            hist_temp_sum = np.zeros(len(edges)-1)
            for i in range(nop):
                hist_temp,crap = np.histogram(SPK_op_ext_aligned[i][s],edges)
                hist_temp_sum += hist_temp
            PSTH.append(hist_temp_sum/(nop*bw))
            PSTH_trim.append(PSTH[s][trim:-trim])
            PSTH_blmn.append(np.mean(PSTH_trim[s][0:int(3/bw)]))
            PSTH_blstd.append(np.std(PSTH_trim[s][0:int(3/bw)]))
    else:
        PSTH = []
        PSTH_trim = [] # trim 2s from start and end of PSTH (where there are missing spikes due to shifting for alignment)
        PSTH_blmn = []
        PSTH_blstd = []
        trim = int(2/bw - 1) # number of indices of PSTH corresponding to 2s
        for o in range(no):
            PSTH_temp = []
            PSTH_trim_temp = []
            PSTH_blmn_temp = []
            PSTH_blstd_temp = []
            for s in range(len(SPK_op_ext_aligned[0][0])):
                hist_temp_sum = np.zeros(len(edges)-1)
                for i in range(nop):
                    hist_temp,crap = np.histogram(SPK_op_ext_aligned[o][i][s],edges)
                    hist_temp_sum += hist_temp
                PSTH_temp.append(hist_temp_sum/(nop*bw))
                PSTH_trim_temp.append(PSTH_temp[s][trim:-trim])
                PSTH_blmn_temp.append(np.mean(PSTH_trim_temp[s][0:int(3/bw)]))
                PSTH_blstd_temp.append(np.std(PSTH_trim_temp[s][0:int(3/bw)]))
            PSTH.append(PSTH_temp)
            PSTH_trim.append(PSTH_trim_temp)
            PSTH_blmn.append(PSTH_blmn_temp)
            PSTH_blstd.append(PSTH_blstd_temp)

    return PSTH, PSTH_trim, PSTH_blmn, PSTH_blstd

def classify_PSTH(PSTH,bw):
    # This function breaks the aligned PSTH into 3 parts:
    
    # 0.4s arround the stimulus onset (alignement point)
    # 0.6s after that (completing first s after stim onset)
    # 2s after that
    # Note: these values are assuming that bw = 0.2s!
    
    
    # The response in each of these time periods is classified as exited (1), inhibbited (-1),
    # or no change from baseline (0). This is represented by class_vec
    
    # Note: PSTH must have clean edges! PSTH is assumed to have even number of points and > 3s before beta onset
    
    # Take 1st 3s as baseline
    blmn = np.mean(PSTH[0:int(3/bw)])
    blstd = np.std(PSTH[0:int(3/bw)])
    
    half = int(len(PSTH)/2)
    
    class_vec = [0,0,0]
    
    # t1 is bin width
    t2 = 0.6 # s after t1
    t3 = 2.2 # s after t2
    
    if np.mean(PSTH[half-1:half+1]) > blmn + 1.3*blstd: # a little stricter criterion for the transient
        class_vec[0] = 1
    if np.mean(PSTH[half-1:half+1]) < blmn - 1.3*blstd:
        class_vec[0] = -1
    if np.mean(PSTH[(half+1):(half+1+int(t2/bw))]) > blmn + blstd:
        class_vec[1] = 1
    if np.mean(PSTH[(half+1):(half+1+int(t2/bw))]) < blmn - blstd:
        class_vec[1] = -1
    if np.mean(PSTH[(half+1+int(t2/bw)):(half+1+int((t2+t3)/bw))]) > blmn + blstd:
        class_vec[2] = 1
    if np.mean(PSTH[(half+1+int(t2/bw)):(half+1+int((t2+t3)/bw))]) < blmn - blstd:
        class_vec[2] = -1
       
    # Naming:
    # [0,0,0] unmodulated (U)
    # [1,0,0] transient excitation (tE)
    # [-1,0,0] transient inhibition (tI)
    # [0,1,0] delayed transient excitation (dtE)
    # [0,-1,0] delayed transient inhibition (dtI)
    # [1,1,1] persistent excitation (pE)
    # [-1,-1,-1] persistent inhibition (pI)
    # [0,0,1],[0,1,1] delayed persistent excitation (dpE)
    # [0,0,-1],[0,-1,-1] delayed persistent inhibition (dpI)
    # [1,-1,0] EI
    # [1,-1,1] EIE
    # [-1,1,0] IE
    # [-1,1,-1] IEI
    # remaining patterns might be ignored
    
    return class_vec

def get_peak_phase_gauss(data,edges):
    from scipy.optimize import curve_fit
    import math

    # Define gaussian
    def gauss(x, *p):
        A, mu, sigma = p
        return A*np.exp(-(x-mu)**2/(2.*sigma**2))

    edge_step = edges[1]-edges[0]
    hist, bin_edges = np.histogram(data, edges)
    bin_centres = edges[0:-1]+edge_step/2

    # p0 is the initial guess for the fitting coefficients (A, mu and sigma above)
    p0 = [np.max(hist), bin_centres[np.argmax(hist)], 0.5]

    try:
        coeff, var_matrix = curve_fit(gauss, bin_centres, hist, p0=p0, maxfev = 1000)
        
        # Get the fitted curve
        hist_fit = gauss(bin_centres, *coeff)

        mse = np.mean(np.square(hist_fit-hist)) # mean sqared error of gaussian fit
        peak_phase = math.degrees(bin_centres[np.argmax(hist_fit)]) # peak phase in degrees
        # since distributions aren't exactly Gaussian I average the fitted peak with the empirical max peak
        #peak_phase = np.round((peak_phase+math.degrees(bin_centres[np.argmax(hist)]))/2,decimals=1)

        peak_phase = np.round(peak_phase,decimals=1)
    except:
        peak_phase = np.round(math.degrees(bin_centres[np.argmax(hist)]),decimals=1)
        mse = np.nan
        hist_fit = np.nan

    
    if peak_phase == 0:
        peak_phase = np.abs(peak_phase) # sometimes 0 is returned as -0.0

    return peak_phase, mse, hist, hist_fit


def get_peak_phase_sinORgauss(data,edges):
    # Fits a sine and a gaussian to a histogram of the spike-phase product values in "data" binned into "edges".
    # Returns the phase at the peak value of the fit with the lowest error.
    # INPUTS:
    # data      -  list of spike-phase products
    # edges     -  edges from -pi to pi for binning data into histogram
    from scipy.optimize import curve_fit
    import math
    
    # Define sine function
    def sinfunc(x, *p):
        #A, w, p, c = p # if w (fq) is not fixed
        A, p, c = p # fixed w
        #return A * np.sin(w*x + p) + c
        return A * np.sin(x + p) + c
    
    # Define gaussian
    def gauss(x, *p):
        A, mu, sigma = p
        return A*np.exp(-(x-mu)**2/(2.*sigma**2))
    
    edge_step = edges[1]-edges[0]
    hist, bin_edges = np.histogram(data, edges)
    bin_centres = edges[0:-1]+edge_step/2
    
    # p0sin is the initial guess for the fitting coefficients (A, w, p, c)
    #p0sin = [np.max(hist), 1, 0, np.max(hist)/2]
    # p0sin is the initial guess for the fitting coefficients (A, p, c)
    p0sin = [np.max(hist)/2, 0, np.max(hist)]
    
    # p0gauss is the initial guess for the fitting coefficients (A, mu and sigma)
    p0gauss = [np.max(hist), bin_centres[np.argmax(hist)], 0.5]
    
    try:
        coeff_sin, var_matrix_sin = curve_fit(sinfunc, bin_centres, hist, p0=p0sin, maxfev = 50000)
        coeff_gauss, var_matrix_gauss = curve_fit(gauss, bin_centres, hist, p0=p0gauss, maxfev = 50000)
        
        # Get the fitted curve
        hist_fit_sin = sinfunc(bin_centres, *coeff_sin)
        hist_fit_gauss = gauss(bin_centres, *coeff_gauss)
        
        mse_sin = np.mean(np.square(hist_fit_sin-hist)) # mean sqared error of sine fit
        mse_gauss = np.mean(np.square(hist_fit_gauss-hist)) # mean sqared error of gaussian fit
        
        if mse_sin < mse_gauss:
            peak_phase = math.degrees(bin_centres[np.argmax(hist_fit_sin)]) # peak phase in degrees
            mse = mse_sin
            hist_fit = hist_fit_sin
        else:
            peak_phase = math.degrees(bin_centres[np.argmax(hist_fit_gauss)]) # peak phase in degrees
            mse = mse_gauss
            hist_fit = hist_fit_gauss
        
        peak_phase = np.round(peak_phase,decimals=1)
    except:
        # if both fits fail to evaluate then the hist is probably bad, but save the peak phase anyways
        peak_phase = np.round(math.degrees(bin_centres[np.argmax(hist)]),decimals=1)
        mse = np.nan
        hist_fit = np.nan
    
    
    if peak_phase == 0:
        peak_phase = np.abs(peak_phase) # sometimes 0 is returned as -0.0
    
    return peak_phase, mse, hist, hist_fit

def calc_DKL(SPHprod,RPHprod,edges,pc):
    # Calculate DKL between real/fake data and uniform distribution for phist.
    # Also calculate the pc'th percentile of the DKL for the shuffles.
    # INPUTS:
    # SPHprod     -   embended list of spike-phase products (length #clusters)
    # RPHprod     -   embended list of shuffled spike-phase products (length #clusters, length #shuffles)
    # edges       -   edges for bining histogram
    # pc          -   percentile of shuffled dkl
    ##########################################################
    
    nbins = len(edges)-1
    nrand_ph = len(RPHprod[0])
    nclust = len(SPHprod) # number of clusters
    eps = 1e-15 # used for replacing 0s in hist
    
    DKL = []
    DKL_rand = []
    DKL_rand_percentile = []
    unif_dist = np.ones(nbins) # uniform distribution
    for i in range(nclust):
        if SPHprod[i].size > 0: # only perform hist if spikes exist
            [sph_hist,bleh]=np.histogram(SPHprod[i],edges)
            # set 0 = eps to avoid Inf
            sph_hist=sph_hist.astype('float') # must convert from int64 to float
            sph_hist[sph_hist==0] = eps # set 0's = eps to avoid dividing by zero in
            DKL.append(stats.entropy(sph_hist,unif_dist))
            dklrand_temp = [] # initialize for storing random DKLs
            for r in range(nrand_ph):
                [sph_hist,bleh]=np.histogram(RPHprod[i][r],edges)
                # set 0 = eps to avoid Inf
                sph_hist=sph_hist.astype('float') # must convert from int64 to float
                sph_hist[sph_hist==0] = eps
                dklrand_temp.append(stats.entropy(sph_hist,unif_dist))
            DKL_rand.append(dklrand_temp)
            DKL_rand_percentile.append(np.percentile(dklrand_temp,pc)) # pc'th percentile
        else:
            DKL.append([])
            DKL_rand.append([])
            DKL_rand_percentile.append([])

    return DKL, DKL_rand, DKL_rand_percentile

def scandtrig_while(dtrig,skipdp):
    '''
    Unfortunately, the TTL pulses sometimes have multiple closely spaced repeats.
    
    This function scans the digital trigger (dtrig) to find the onsets of each
    repeated pulse train, then skips a time skipdp (in samples) and starts
    scanning again until no more triggers are detected
    
    NOTE: This function is meant to analyze OpenEphys event data triggered by
    MedPC TTL.
    
    NOTE: skipdp must be big enough that the algorithm skips over the repeats
    ,but small enough that it does not skip too far and miss the next real pulse!
    
    Boleszek Osinski (2017)
    Kay Lab, University of Chicago
    '''
    
    dtrig_clean = [dtrig[0]] # start with first pulse
    ind = dtrig_clean # initialize
    ii = 0
    while len(ind)>0:
        # prev_ind will update each time new pulse is detected
        prev_ind = dtrig_clean[ii]
        # start scan skipdp after prev_ind to skip over the transients
        ind = np.where(dtrig>(prev_ind+skipdp))[0]
        if len(ind)>0:
            dtrig_clean.append(dtrig[ind[0]]) # append the 1st pulse at index ind[0]
        ii = ii+1
        
    return dtrig_clean